# -*- coding: utf-8 -*-
"""ProyectoMl.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tDUI3JJiLBR_96j-qm198-0vdMCaqi2T

Importación de librerias a usar
"""

import os
import re
import pandas as pd
from PyPDF2 import PdfReader
import pandas as pd
from limpiar_proceso import clean_text
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
import redis
import json
import pickle

r = redis.StrictRedis(host='localhost', port=6379, db=0)

# Función para extraer texto de un PDF
def extraer_texto(pdf_path):
    texto = ""
    with open(pdf_path, "rb") as file:
        reader = PdfReader(file)
        for page in reader.pages:
            texto += page.extract_text()
    return texto

# Función para encontrar títulos y el texto entre ellos
def encontrar_titulos_y_texto(texto):
    # Patrón para detectar los títulos
    patron_titulo = r'(\d{2}/\d{2}/\d{4} \d{2}:\d{2} [A-ZÁÉÍÓÚÑ\s]+ \([A-ZÁÉÍÓÚÑ\s]+\))'
    bloques = re.split(patron_titulo, texto)
    # Eliminar el primer elemento vacío
    bloques = bloques[1:]
    # Agrupar títulos y texto entre ellos
    bloques = [bloques[i:i+2] for i in range(0, len(bloques), 2)]
    return bloques

# Función para eliminar fechas, números y caracteres no alfanuméricos
def eliminar_fechas_numeros_no_alfanumericos(texto):
    patron_fechas = r'\d{2}/\d{2}/\d{4}'  # Coincide con fechas en formato dd/mm/yyyy
    texto_sin_fechas = re.sub(patron_fechas, '', texto)

    patron_numeros = r'\d+'  # Coincide con números
    texto_sin_numeros = re.sub(patron_numeros, '', texto_sin_fechas)

    texto_filtrado = re.sub(r'[^a-zA-ZáéíóúÁÉÍÓÚñÑ\s]', '', texto_sin_numeros)
    return texto_filtrado

# Función para eliminar los pies de página
def eliminar_pie_pagina(texto):
    # Patrón para eliminar el pie de página
    patron_pie_pagina = r'Página \d+ de \d+'
    texto_sin_pie = re.sub(patron_pie_pagina, '', texto)
    return texto_sin_pie

# Función para filtrar la información del veredicto
def filter_verdict_information(texto):
    # Patrones para excluir información sobre el veredicto
    patrones_veredicto = [
        r'culpable',  # coincide con "culpable"
        r'inocente',  # coincide con "inocente"
        r'verdicto',  # coincide con "veredicto"
        r'sentencia',  # coincide con "sentencia"
        r'condena',  # coincide con "condena"
        r'absuelto',  # coincide con "absuelto"
        r'libertad',  # coincide con "libertad"
        r'culpabilidad',  # coincide con "culpabilidad"
        r'inocencia'  # coincide con "inocencia"
        # Agregar más patrones según sea necesario
    ]

    # Patrón para eliminar correos electrónicos
    patron_correo = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'

    # Combinar los patrones en una sola expresión regular
    patron_combinado = re.compile('|'.join(patrones_veredicto + [patron_correo]), re.IGNORECASE)

    # Eliminar todas las coincidencias
    texto_filtrado = patron_combinado.sub('', texto)

    return texto_filtrado

# Directorio con archivos PDF
pdf_directory = "/home/administrador/Documentos/ProyectoMLP1/Documentos"

# Listar todos los archivos PDF en el directorio
pdf_files = [f for f in os.listdir(pdf_directory) if f.endswith('.pdf')]

data = []

# Procesar cada archivo PDF
for pdf_file in pdf_files:
    pdf_path = os.path.join(pdf_directory, pdf_file)
    # Extraer texto del PDF
    texto_pdf = extraer_texto(pdf_path)

    # Encontrar títulos y el texto entre ellos
    titulos_y_texto = encontrar_titulos_y_texto(texto_pdf)
    for titulo, texto in titulos_y_texto:
        texto_sin_pie = eliminar_pie_pagina(texto)
        texto_filtrado = filter_verdict_information(texto_sin_pie)
        texto_final = eliminar_fechas_numeros_no_alfanumericos(texto_filtrado)
        data.append((pdf_file, titulo, texto_final.strip()))

# Crear un DataFrame y guardarlo en un archivo CSV
df = pd.DataFrame(data, columns=['NombreArchivo', 'Título', 'Texto'])
df.to_csv('documento_procesado.csv', index=False)


"""Convirtiendo los documentos a un DataFrame por secciones"""

# Seleccionar filas donde la columna 'Título' contiene una palabra específica

# Convertir la columna 'NombreArchivo' a string
df['NombreArchivo'] = df['NombreArchivo'].astype(str)

# Filtrar filas que contienen 'RESOLUCION' o 'AUTO RESOLUTIVO' en la columna 'Título'
condicion = df['Título'].str.contains('RESOLUCION') | df['Título'].str.contains('AUTO RESOLUTIVO')
filas_seleccionadas = df[condicion]

# Función para asignar sentencia basada en el nombre del archivo
def asignar_sentencia(nombre_archivo):
    numero = int(nombre_archivo.split('.')[0])
    return 1 if 0 <= numero <= 9 else 0

# Unir textos de filas con títulos repetidos
filas_unidas = df.groupby('NombreArchivo')['Texto'].apply(' '.join).reset_index()

# Asignar valores de sentencia
filas_unidas['sentencia'] = filas_unidas['NombreArchivo'].apply(asignar_sentencia)

# Extraer la parte numérica del nombre del archivo para la ordenación
filas_unidas['NombreArchivo_num'] = filas_unidas['NombreArchivo'].str.extract(r'(\d+)', expand=False).astype(int)

# Ordenar el DataFrame 'filas_unidas' por 'NombreArchivo_num'
filas_unidas = filas_unidas.sort_values(by='NombreArchivo_num').reset_index(drop=True)

# Eliminar la columna auxiliar 'NombreArchivo_num'
filas_unidas.drop(columns='NombreArchivo_num', inplace=True)

# Guardar las filas unidas en una variable
filas_unidas_var = filas_unidas['Texto']

# Convertir la columna 'NombreArchivo' a un tipo que pandas puede ordenar correctamente en el DataFrame original
df['NombreArchivo_num'] = df['NombreArchivo'].str.extract(r'(\d+)', expand=False).astype(int)

# Ordenar el DataFrame por 'NombreArchivo_num'
df_sorted = df.sort_values(by='NombreArchivo_num').reset_index(drop=True)

# Convertir 'NombreArchivo_num' de nuevo a formato original con '.pdf'
df_sorted['NombreArchivo'] = df_sorted['NombreArchivo_num'].astype(str) + '.pdf'

# Eliminar la columna auxiliar 'NombreArchivo_num'
df_sorted.drop(columns='NombreArchivo_num', inplace=True)

# Imprimir el DataFrame ordenado y con la columna 'sentencia'
print(filas_unidas)

# Serializar el DataFrame final y guardarlo en Redis
df_final_serializado = pickle.dumps(filas_unidas)
r.set('documento_procesado_final', df_final_serializado)

# Recuperar el DataFrame final de Redis (opcional)
df_recuperado_serializado = r.get('documento_procesado_final')
df_recuperado = pickle.loads(df_recuperado_serializado)

d = clean_text(*filas_unidas_var)

"""Prediccion del modelo"""
# Supongamos que tienes dos listas de textos preprocesados: documentos_culpables y documentos_inocentes
documentos_culpables = filas_unidas.loc[0:9, 'Texto'].tolist()  # Llena esto con tus textos preprocesados de documentos culpables
documentos_inocentes = filas_unidas.loc[10:, 'Texto'].tolist()  # Llena esto con tus textos preprocesados de documentos inocentes

cdc = clean_text(*documentos_culpables)
cdi = clean_text(*documentos_inocentes)

# Guardar la lista en Redis
r.set('tokens_culpables', json.dumps(cdc))
r.set('tokens_inocentes', json.dumps(cdi))

# Recuperar la lista de Redis
inocentes = json.loads(r.get('tokens_inocentes'))
culpables = json.loads(r.get('tokens_culpables'))

#Unir ambos conjuntos para ajustar el vectorizador a todos los documentos
todos_los_documentos =  cdc  + inocentes 

# # Crear un TF-IDF vectorizer y ajustar a todos los documentos
vectorizer = TfidfVectorizer()  # Puedes ajustar el número de características según sea necesario
vectorizer.fit(todos_los_documentos)
# Transformar cada conjunto de documentos

# # Transformar cada conjunto de documentos
tfidf_culpables = vectorizer.transform(documentos_culpables)
tfidf_inocentes = vectorizer.transform(documentos_inocentes)

# Obtener los nombres de las características
palabras = vectorizer.get_feature_names_out()

# Convertir las matrices TF-IDF a DataFrames para mejor manejo
df_tfidf_culpables = pd.DataFrame(tfidf_culpables.toarray(), columns=palabras)
df_tfidf_inocentes = pd.DataFrame(tfidf_inocentes.toarray(), columns=palabras)

# Promediar los TF-IDF para cada palabra en ambos grupos
promedio_tfidf_culpables = df_tfidf_culpables.mean(axis=0)
promedio_tfidf_inocentes = df_tfidf_inocentes.mean(axis=0)


# Convertir Series a diccionarios
promedio_tfidf_culpables_dict = promedio_tfidf_culpables.to_dict()
promedio_tfidf_inocentes_dict = promedio_tfidf_inocentes.to_dict()

# Guardar los diccionarios en Redis
r.set('promedio_tfidf_culpables', json.dumps(promedio_tfidf_culpables_dict))
r.set('promedio_tfidf_inocentes', json.dumps(promedio_tfidf_inocentes_dict))

# Para recuperar los datos desde Redis
promedio_tfidf_culpables_recuperado = json.loads(r.get('promedio_tfidf_culpables'))
promedio_tfidf_inocentes_recuperado = json.loads(r.get('promedio_tfidf_inocentes'))

# Convertir los datos recuperados a DataFrame si es necesario
promedio_tfidf_culpables_df = pd.DataFrame.from_dict(promedio_tfidf_culpables_recuperado, orient='index')
promedio_tfidf_inocentes_df = pd.DataFrame.from_dict(promedio_tfidf_inocentes_recuperado, orient='index')

x = promedio_tfidf_culpables_df[0].values
y = promedio_tfidf_inocentes_df[0].values
import os
import re
import pandas as pd
from PyPDF2 import PdfReader
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer

# Función para extraer texto de un PDF
def extraer_texto(pdf_path):
    texto = ""
    with open(pdf_path, "rb") as file:
        reader = PdfReader(file)
        for page in reader.pages:
            texto += page.extract_text()
    return texto


# Crear un vectorizador de conteo para convertir tokens en una matriz
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(inocentes + culpables)

# Función para calcular la similitud coseno entre un nuevo documento y los promedios TF-IDF de culpables e inocentes
def calcular_similitud(nuevo_documento, vectorizer, promedio_tfidf_culpables, promedio_tfidf_inocentes):
    # Preprocesar el nuevo documento
    nuevo_documento_procesado = clean_text(nuevo_documento)[0]  # Ajustar para procesar un solo documento

    # Transformar el documento usando el vectorizador TF-IDF
    nuevo_documento_tfidf = vectorizer.transform([nuevo_documento_procesado])

    # Calcular la similitud coseno con los promedios de culpables e inocentes
    similitud_culpables = cosine_similarity(nuevo_documento_tfidf, [promedio_tfidf_culpables])
    similitud_inocentes = cosine_similarity(nuevo_documento_tfidf, [promedio_tfidf_inocentes])
    es_culpable = similitud_culpables.mean() > similitud_inocentes.mean()
    return es_culpable

# Función para clasificar un nuevo documento
def clasificar_documento(nuevo_documento, vectorizer, promedio_tfidf_culpables, promedio_tfidf_inocentes):
    similitud_culpables = calcular_similitud(nuevo_documento, vectorizer, promedio_tfidf_culpables, promedio_tfidf_inocentes)
    return "El documento es culpable." if similitud_culpables  else "El documento es inocente."

# Ejemplo de uso: ingresa la ruta de tu nuevo documento PDF aquí
ruta_nuevo_documento = "expel_07710202000292_22278534_02062024.pdf"
texto_nuevo_documento = extraer_texto(ruta_nuevo_documento)
resultado = clasificar_documento(texto_nuevo_documento, vectorizer, x, y)
print(f"El documento es: {resultado}")